---
layout: post
title: 'Resnet论文翻译'
date: 2020-12-28
author: AugustMe
cover: 'http://on2171g4d.bkt.clouddn.com/jekyll-banner.png'
tags: 论文翻译
---

> 深度残差学习用于图像识别.

### 摘要

更深的神经网络更难训练。我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。在ImageNet数据集上我们评估了深度高达152层的残差网络——比VGG深8倍但仍具有较低的复杂度。这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。这个结果在ILSVRC 2015分类任务上赢得了第一名。我们也在CIFAR-10上分析了100层和1000层的残差网络。

对于许多视觉识别任务而言，表示的深度是至关重要的。仅由于我们非常深度的表示，我们便在COCO目标检测数据集上得到了28%的相对提高。深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。

### 1. 介绍

深度卷积神经网络导致了图像分类的一系列突破。深度网络自然地将低/中/高级特征和分类器以端到端多层方式进行集成，特征的“级别”可以通过堆叠层的数量（深度）来丰富。最近的证据显示网络深度至关重要，在具有挑战性的ImageNet数据集上领先的结果都采用了“非常深”的模型，深度从16到30之间。许多其它重要的视觉识别任务也从非常深的模型中得到了极大受益。

在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸这个众所周知的问题，它从一开始就阻碍了收敛。然而，这个问题通过标准初始化和中间标准化层在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。

当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差，正如[11, 42]中报告的那样，并且由我们的实验完全证实。图1显示了一个典型的例子。

![input and output for a random image in the test dataset](http://noahsnail.com/images/resnet/Figure_1.jpeg)

图1 20层和56层的“简单”网络在CIFAR-10上的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。ImageNet上的类似现象如图4所示。
